# ğŸ§‘â€ğŸ’¼ IBM HR Attrition â€” PredicciÃ³n de RotaciÃ³n de Empleados

Proyecto de Machine Learning para predecir quÃ© empleados tienen mayor probabilidad de abandonar la empresa, usando el dataset pÃºblico **IBM HR Analytics Employee Attrition & Performance**.

---

## ğŸ“‹ DescripciÃ³n

La rotaciÃ³n de empleados (*attrition*) es uno de los mayores costes ocultos para las organizaciones. Este proyecto entrena y evalÃºa modelos de clasificaciÃ³n para identificar los factores clave que llevan a un empleado a dejar la empresa, permitiendo a RRHH actuar de forma preventiva.

Se comparan dos aproximaciones:
- **Random Forest** â€” modelo de ensamble robusto ante variables irrelevantes y no linealidades.
- **RegresiÃ³n LogÃ­stica** â€” modelo interpretable, Ãºtil para entender el peso de cada variable.

---

## ğŸ—‚ï¸ Estructura del proyecto

```
proyecto-IBMAttrition/
â”‚
â”œâ”€â”€ data/                   # Dataset original y versiones procesadas
â”œâ”€â”€ models/                 # Modelos entrenados serializados (.pkl / .joblib)
â”œâ”€â”€ src/                    # CÃ³digo fuente modular
â”‚   â”œâ”€â”€ data_loader.py           # Lectura y limpieza de datos
â”‚   â”œâ”€â”€ evaluation.py            # Funciones para mÃ©tricas y grÃ¡ficos
â”‚   â”œâ”€â”€ extractors.py            # Class extractor 
â”‚   â”œâ”€â”€ feature_engeneering.py   # Creacion de variables nuevas
â”‚   â”œâ”€â”€ loader.py                # Class load
â”‚   â”œâ”€â”€ parser.py                # Parseador del config.yaml
â”‚   â”œâ”€â”€ processing.py            # Preprocesado de los datos
â”‚   â””â”€â”€ train.py                 # entrenamiento para los dos modelos
â”‚
â”œâ”€â”€ 01_eda.ipynb            # AnÃ¡lisis exploratorio de datos (EDA)
â”œâ”€â”€ 02_entreno_rf.ipynb     # Entrenamiento y evaluaciÃ³n â€” Random Forest
â”œâ”€â”€ 03_entreno_rl.ipynb     # Entrenamiento y evaluaciÃ³n â€” RegresiÃ³n LogÃ­stica
â”‚
â”œâ”€â”€ main.py                 # Script principal â€” ejecuta el pipeline completo
â”œâ”€â”€ config.yaml             # ConfiguraciÃ³n centralizada (rutas, hiperparÃ¡metros)
â”œâ”€â”€ pyproject.toml          # Dependencias del proyecto (gestionadas con uv)
â””â”€â”€ uv.lock                 # Lockfile de dependencias
```

> **Nota de diseÃ±o:** Los notebooks (`01`, `02`, `03`) estÃ¡n pensados como herramienta de anÃ¡lisis visual y exploraciÃ³n. El entrenamiento real del modelo se ejecuta mediante un pipeline , que importa las funciones de `src/` y lee la configuraciÃ³n de `config.yaml`.

---

## ğŸš€ InstalaciÃ³n y uso

### 1. Clonar el repositorio

```bash
git clone https://github.com/estebanSem/proyecto-IBMAttrition.git
cd proyecto-IBMAttrition
```

### 2. Instalar dependencias

Este proyecto usa [uv](https://github.com/astral-sh/uv) como gestor de paquetes:

```bash
pip install uv
uv sync
```

### 3. Ejecutar el pipeline completo

```
Seguir los notebooks
```

Esto leerÃ¡ la configuraciÃ³n de `config.yaml`, procesarÃ¡ los datos, entrenarÃ¡ los modelos y guardarÃ¡ los resultados en `models/`.

### 4. Explorar los notebooks (opcional)

```bash
jupyter notebook
```

| Notebook | DescripciÃ³n |
|---|---|
| `01_eda.ipynb` | AnÃ¡lisis exploratorio: distribuciones, correlaciones, clase desbalanceada |
| `02_entreno_rf.ipynb` | Entrenamiento y mÃ©tricas del modelo Random Forest |
| `03_entreno_rl.ipynb` | Entrenamiento y mÃ©tricas del modelo RegresiÃ³n LogÃ­stica |

---

## ğŸ“Š Dataset

**IBM HR Analytics Employee Attrition & Performance**

- **Fuente:** [Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)
- **Registros:** 1.470 empleados
- **Target:** `Attrition` (Yes / No) â€” variable binaria
- **Features:** 35 variables entre demogrÃ¡ficas, laborales y de satisfacciÃ³n (edad, departamento, horas extra, nivel de satisfacciÃ³n, salario, etc.)

> âš ï¸ El dataset presenta **desbalance de clases** (~84% No attrition vs ~16% Yes). Esto se tiene en cuenta durante el entrenamiento.

---

## ğŸ¤– Modelos

### Random Forest
- Modelo de ensamble basado en Ã¡rboles de decisiÃ³n
- Robusto ante outliers y variables irrelevantes
- Permite obtener importancia de features

### RegresiÃ³n LogÃ­stica
- Modelo lineal interpretable
- Ãštil para entender el impacto individual de cada variable
- RÃ¡pido de entrenar y fÃ¡cil de explicar a negocio

---

## ğŸ“ˆ MÃ©tricas de evaluaciÃ³n

Dado el desbalance de clases, las mÃ©tricas principales son:

- **F1-Score** (clase minoritaria)
- **ROC-AUC**
- **Precision / Recall**
- **Matriz de confusiÃ³n**

---

## âš™ï¸ ConfiguraciÃ³n

Todos los parÃ¡metros del pipeline se encuentran en `config.yaml`, incluyendo rutas de datos, hiperparÃ¡metros de los modelos y configuraciÃ³n del preprocesamiento. Modifica este archivo para ajustar el comportamiento sin tocar el cÃ³digo.


---

## ğŸ‘¤ Autor

**Esteban Sempere** â€” [@estebanSem](https://github.com/estebanSem)
